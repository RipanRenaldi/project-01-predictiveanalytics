# -*- coding: utf-8 -*-
"""project1-predictive-analysis-laptop-price.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IxyB7-gt8mV4fFO2U1S3SWa2KP7EOVIM
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor
from sklearn.linear_model import LinearRegression
import numpy as np
import seaborn as sns

df = pd.read_csv("/content/laptop_data.csv")
df

df.info()

# data di atas memiliki 23 kolom dan 896 baris, dengan tipe data 7 int, 15 object, 1 float.
# mari cek missing value
df.isna().sum()

"""sekilas data diatas tidak memiliki missing value, namun jika kita lihat data yang ada, pada kolom display_size memiliki value "Missing" yang berarti missing value pada data"""

#cek data duplikat
df.duplicated().sum()

# hapus duplikat
df = df.drop_duplicates()
df

"""Mari kita ubah missing value menjadi NaN terlebih dahulu"""

df = df.replace("Missing",np.nan)
df

"""cek missing value

"""

df.isna().sum()

"""kita memiliki 23 column, sehingga tidak apa jika kita hapus kolom yang memiliki missing value terbanyak. Terlepas dari kolom yang kita punya, dataset kita masih memiliki informasi yang cukup

"""

df = df.drop(columns=["display_size"])
df

"""# Exploratory Data Analisis"""

# Univariate analisis
# Brand apa yang paling banyak muncul pada data
counts = df["brand"].value_counts()
counts.plot(kind="bar") # Asus merupakan brand yang paling banyak pada data

# berapa ukuran ram yang digunakan pada laptop?
counts = df["ram_gb"].value_counts()
counts.plot(kind="bar")

# merk processor apa saja yang digunakan pada laptop
counts = df["processor_brand"].value_counts()
counts.plot(kind="bar")

# Generasi processor yang digunakan pada laptop
counts = df["processor_gnrtn"].value_counts()
counts.plot(kind="bar") # kesimpulan yang didapatkan yakni bahwa laptop yanmg digunakan kebanyakan laptop terbaru, dengan chipset processor
                        # generasi 10 dan 11

# Jenis-jenis ukuran kapasitas ssd yang digunakan pada laptop
counts = df["ssd"].value_counts()
counts.plot(kind="bar")

# jenis-jenis ukuran kapasitas hdd yang digunakan pada laptop
counts = df["hdd"].value_counts()
counts.plot(kind="bar")

# apakah laptop-laptop yang ada menerapkan fitur Touchscreen?
counts = df["Touchscreen"].value_counts()
counts.plot(kind="bar")

# jenis processor yang digunakan pada laptop
counts = df["processor_name"].value_counts()
counts.plot(kind="bar") # kebanyakan processor pada laptop menggunakan intel core i5

# berapa banyak laptop yang telah terinstall msoffice 
counts = df["msoffice"].value_counts()
counts.plot(kind="bar")

# apa saja type ram yang digunakan pada laptop
counts = df["ram_type"].value_counts()
counts.plot(kind="bar")

df.columns

# jenis ukuran graphic_card / VGA dalam satuan GB yang digunakan pada laptop
counts = df["graphic_card_gb"].value_counts()
counts.plot(kind="bar") # kurang dari 30% data yang ada menggunakan vga dengan ukuran 2,4,6,8 GB

"""## Menangani Missing value dengan teknik arbitrary imputation"""

# Mengingat dataset yang kita miliki berjumlah sedikit, sehingga saya penerapan teknik imputation yang lain agak dilema..
# seperti imputation mean, median, modus, zero value. Sehingga saya akan menggunakan teknik imputasi arbitrary suka-suka, dengan tujuan menjadikan
# nilai imputasi tersebut menjadi nilai tersendiri

df.isna().sum()

df = df.fillna("x")
df

df["graphic_card_gb"]

df.isna().sum()

"""## Membersihkan data

"""

# menghilangkan karakter string pada data sekaligus mengubahnya menjadi tipe data number
df["ram_gb"] = df["ram_gb"].str.replace("GB GB","")
df["ram_gb"] = pd.to_numeric(df["ram_gb"])

df["ssd"] = df["ssd"].str.replace("GB","")
df["ssd"] = pd.to_numeric(df["ssd"])

df["hdd"] = df["hdd"].str.replace("GB","")
df["hdd"] = pd.to_numeric(df["hdd"])
df

df["os_bit"] = df["os_bit"].str.replace("-bit","")
df["os_bit"] = pd.to_numeric(df["os_bit"])

"""## Melihat korelasi label latest_price terhadap fitur kategorical"""

# Melihat korelasi fitur latestprice terhadap fitur kategorical
cat_feature = df.select_dtypes(include="object")
cat_feature = cat_feature.columns

for column in cat_feature:
  sns.catplot(x=column, y="latest_price", data=df, kind="bar")

"""## Menangani data ordinal atau data yang berurutan dengan label encoder

"""

# data ordinal atau data yang berurutan mencakup kolom processor_gnrtn, dan ram_type
le = LabelEncoder()
df["processor_gnrtn"] = le.fit_transform(df["processor_gnrtn"])
df["ram_type"] = le.fit_transform(df["ram_type"])
df

"""## Menangani binary data dengan mengubah "yes" menjadi 1, dan "no" jadi 0"""

df.info()

df["Touchscreen"] = df["Touchscreen"].apply(lambda x: 1 if x == "Yes" else(0 if x == "No" else None))
df["msoffice"] = df["msoffice"].apply(lambda x: 1 if x == "Yes" else(0 if x == "No" else None))
df

df["os_bit"] = df["os_bit"].apply(lambda x: 1 if x == 64 else (0 if x == 32 else None))
df

"""## Menangani data nominal atau data yang tidak berurutan dengan one hot encoding

"""

# nominal data terletak pada kolom, brand, model, processor_brand, processor_name, os, weight, 
category = pd.get_dummies(df[["brand","model","processor_brand","processor_name","os","weight"]])
category

saved_df = df.copy()

new_df = pd.concat([df, category],axis=1)
new_df

# drop kolom original
new_df = new_df.drop(columns=["brand","model","processor_brand","processor_name","os","weight"])
new_df

"""## Membagi data latih dan data uji

"""

fitur = new_df.drop(columns=["latest_price"])
label = new_df["latest_price"]

X_train, X_test, Y_train, Y_test = train_test_split(fitur, label, test_size=.2, random_state=42)

"""## Modeling menggunakan beberapa algoritma"""

rf = RandomForestRegressor(n_estimators=80, max_depth=16)
rf.fit(X_train, Y_train)

mae = mean_absolute_error(Y_train, rf.predict(X_train))
mae

plt.scatter(Y_train, rf.predict(X_train))

prediksi = rf.predict(X_test)
hasil = {"Label" : Y_test, "hasil_prediksi" : prediksi}
pd.DataFrame(hasil)

rf.score(X_test,Y_test)

boosting = AdaBoostRegressor(n_estimators = 50, learning_rate= 0.005)
boosting.fit(X_train, Y_train)

boosting_mae = mean_absolute_error(Y_test, boosting.predict(X_test))
boosting_mae

linreg = LinearRegression()
linreg.fit(X_train, Y_train)

linreg_mae = mean_absolute_error(Y_train, linreg.predict(X_train))
linreg_mae

plt.scatter(Y_train, linreg.predict(X_train))

model_dict = {"rf" : rf, "boosting" : boosting, "lr" : linreg}
model_mae = pd.DataFrame(index=["rf","boosting","lr"], columns=["train_mae","test_mae"])
for x,y in model_dict.items():
  model_mae.loc[x,"train_mae"] = mean_absolute_error(Y_train, y.predict(X_train))
  model_mae.loc[x,"test_mae"] = mean_absolute_error(Y_test, y.predict(X_test))

model_mae

model_mae.plot(kind="bar")

"""### Terlihat bahwa algoritma random forest memiliki mae yang paling kecil, sehingga model inilah yang optimal untuk dataset prediksi harga laptop

#Improve model dengan hyperparameter tuning menggunakan GridSearchCV
"""

grid_params = {"n_estimators" : [50,60,70,80,90,100], "max_depth" : [4,8,16,32,64]}
grid = GridSearchCV(RandomForestRegressor(), grid_params, cv=5, verbose=2, scoring="neg_mean_absolute_error")
grid.fit(X_train, Y_train)

print(grid.best_params_)

"""## Melatih ulang model menggunakan parameter terbaik"""

model = RandomForestRegressor(n_estimators=100, max_depth=32)
model.fit(X_train, Y_train)

model_mae.loc["rf","train_mae"] = mean_absolute_error(Y_train, model.predict(X_train))
model_mae.loc["rf","test_mae"] = mean_absolute_error(Y_test, model.predict(X_test))
model_mae

prediksi = {"Y_true" : Y_test[:2]}
coba = X_test.iloc[:2].copy()

prediksi["random_forest"] = model.predict(coba).round(1)
pd.DataFrame(prediksi)